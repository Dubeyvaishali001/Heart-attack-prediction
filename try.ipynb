{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Data Overview:\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope   \n",
      "0   63    1   3       145   233    1        0      150      0      2.3      0  \\\n",
      "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
      "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
      "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
      "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   0     1       1  \n",
      "1   0     2       1  \n",
      "2   0     2       1  \n",
      "3   0     2       1  \n",
      "4   0     2       1  \n",
      "\n",
      "ðŸ“Œ Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n",
      "None\n",
      "\n",
      "ðŸ“Œ Summary Statistics:\n",
      "              age         sex          cp    trestbps        chol         fbs   \n",
      "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000  \\\n",
      "mean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \n",
      "std      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \n",
      "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
      "25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n",
      "50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n",
      "75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \n",
      "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
      "\n",
      "          restecg     thalach       exang     oldpeak       slope          ca   \n",
      "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000  \\\n",
      "mean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \n",
      "std      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \n",
      "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
      "50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n",
      "75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
      "max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n",
      "\n",
      "             thal      target  \n",
      "count  303.000000  303.000000  \n",
      "mean     2.313531    0.544554  \n",
      "std      0.612277    0.498835  \n",
      "min      0.000000    0.000000  \n",
      "25%      2.000000    0.000000  \n",
      "50%      2.000000    1.000000  \n",
      "75%      3.000000    1.000000  \n",
      "max      3.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"hdp_data.csv\")\n",
    "\n",
    "# Show basic info\n",
    "print(\"ðŸ“Œ Data Overview:\")\n",
    "print(df.head())  # Show first 5 rows\n",
    "\n",
    "print(\"\\nðŸ“Œ Column Info:\")\n",
    "print(df.info())  # Show column types and null values\n",
    "\n",
    "print(\"\\nðŸ“Œ Summary Statistics:\")\n",
    "print(df.describe())  # Check numerical stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data Split & Scaling Completed\n",
      "X_train shape: (242, 13)\n",
      "X_test shape: (61, 13)\n",
      "y_train shape: (242,)\n",
      "y_test shape: (61,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load dataset (assuming 'df' is already loaded)\n",
    "# df = pd.read_csv(\"your_data.csv\")  # Uncomment if loading from a file\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\"]\n",
    "numerical_cols = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "\n",
    "# Split X (features) and y (target)\n",
    "X = df.drop(columns=[\"target\"])  # Features\n",
    "y = df[\"target\"]  # Target\n",
    "\n",
    "# Convert categorical columns to integer dtype (for embedding layers)\n",
    "X[categorical_cols] = X[categorical_cols].astype(int)\n",
    "\n",
    "# Split data into train & test (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Normalize numerical data separately\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test_scaled = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "# Convert back to DataFrame and reassign numerical values\n",
    "X_train[numerical_cols] = X_train_scaled\n",
    "X_test[numerical_cols] = X_test_scaled\n",
    "\n",
    "# Save to CSV for verification\n",
    "X_train.to_csv(\"X_train.csv\", index=False)\n",
    "X_test.to_csv(\"X_test.csv\", index=False)\n",
    "y_train.to_csv(\"y_train.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)\n",
    "\n",
    "# Show shape of datasets\n",
    "print(\"âœ… Data Split & Scaling Completed\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# ð—”ð—±ð—± ð˜ð—µð—¶ð˜€ ð˜ð—¼ ð—¦ð—”ð—©ð—˜ ð˜ð—µð—² ð˜€ð—°ð—®ð—¹ð—²ð—¿:\n",
    "joblib.dump(scaler, 'scaler.pkl') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get number of unique values for each categorical column\n",
    "embedding_dims = {col: X_train[col].nunique() for col in categorical_cols}  \n",
    "\n",
    "class TabTransformerGRU(nn.Module):\n",
    "    def __init__(self, categorical_cols, numerical_cols, embedding_dims, hidden_size=64, output_size=1):\n",
    "        super(TabTransformerGRU, self).__init__()\n",
    "\n",
    "        # Create embeddings for categorical features\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_categories, 10)  # Fixed embedding size of 10\n",
    "            for num_categories in embedding_dims.values()\n",
    "        ])  #dense vector representation\n",
    "\n",
    "        # GRU Layer\n",
    "        input_size = sum(10 for _ in embedding_dims) + len(numerical_cols)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X_cat, X_num):\n",
    "        # Compute categorical embeddings\n",
    "        embedded = [embed(X_cat[:, i]) for i, embed in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embedded, dim=-1)  # Shape: (batch_size, total_embedding_dim)\n",
    "\n",
    "        # Ensure numerical features have correct shape\n",
    "        if X_num.dim() == 1:\n",
    "            X_num = X_num.unsqueeze(1)\n",
    "\n",
    "        # Concatenate categorical embeddings with numerical features\n",
    "        X_combined = torch.cat([embedded, X_num], dim=-1)  # Shape: (batch_size, input_size)\n",
    "\n",
    "        # Reshape for GRU (batch_size, sequence_length=1, input_size)\n",
    "        X_combined = X_combined.unsqueeze(1)\n",
    "\n",
    "        # Pass through GRU\n",
    "        _, hidden = self.gru(X_combined)\n",
    "\n",
    "        # Fully connected output\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output.squeeze(1)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = TabTransformerGRU(categorical_cols, numerical_cols, embedding_dims).to(device)\n",
    "\n",
    "# Assuming y_train contains the class labels\n",
    "class_counts = np.array([count for _, count in sorted(Counter(y_train).items())])\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = torch.tensor(1.0 / class_counts, dtype=torch.float32)\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Define weighted loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert categorical and numerical data to tensors\n",
    "X_train_cat = torch.tensor(X_train[categorical_cols].values, dtype=torch.long)\n",
    "X_train_num = torch.tensor(X_train[numerical_cols].values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)  # Make it (batch_size, 1)\n",
    "\n",
    "X_test_cat = torch.tensor(X_test[categorical_cols].values, dtype=torch.long)\n",
    "X_test_num = torch.tensor(X_test[numerical_cols].values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_cat, X_train_num, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_cat, X_test_num, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert categorical and numerical features to tensors\n",
    "X_cat_tensor = torch.tensor(X_train[categorical_cols].values, dtype=torch.long).to(device)\n",
    "X_num_tensor = torch.tensor(X_train[numerical_cols].values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = TensorDataset(X_cat_tensor, X_num_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6490\n",
      "Epoch 2/20, Loss: 0.5619\n",
      "Epoch 3/20, Loss: 0.4882\n",
      "Epoch 4/20, Loss: 0.4357\n",
      "Epoch 5/20, Loss: 0.3852\n",
      "Epoch 6/20, Loss: 0.3755\n",
      "Epoch 7/20, Loss: 0.3393\n",
      "Epoch 8/20, Loss: 0.3312\n",
      "Epoch 9/20, Loss: 0.3357\n",
      "Epoch 10/20, Loss: 0.3196\n",
      "Epoch 11/20, Loss: 0.3155\n",
      "Epoch 12/20, Loss: 0.2958\n",
      "Epoch 13/20, Loss: 0.2939\n",
      "Epoch 14/20, Loss: 0.3119\n",
      "Epoch 15/20, Loss: 0.2932\n",
      "Epoch 16/20, Loss: 0.2881\n",
      "Epoch 17/20, Loss: 0.2826\n",
      "Epoch 18/20, Loss: 0.3090\n",
      "Epoch 19/20, Loss: 0.2815\n",
      "Epoch 20/20, Loss: 0.2757\n",
      "Training Complete! ðŸŽ‰\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_cat_batch, X_num_batch, y_batch in train_loader:\n",
    "        X_cat_batch, X_num_batch, y_batch = X_cat_batch.to(device), X_num_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(X_cat_batch, X_num_batch)  # Forward pass\n",
    "\n",
    "        loss = loss_fn(outputs, y_batch)  # Compute loss\n",
    "\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete! ðŸŽ‰\")\n",
    "\n",
    "\n",
    "# 2ï¸âƒ£ Save the trained model after training is complete\n",
    "torch.save(model.state_dict(), \"mymodel.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8525\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "X_cat_test_tensor = torch.tensor(X_test[categorical_cols].values, dtype=torch.long).to(device)\n",
    "X_num_test_tensor = torch.tensor(X_test[numerical_cols].values, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_cat_test_tensor, X_num_test_tensor)\n",
    "    predictions = torch.sigmoid(predictions)  # Convert to probabilities\n",
    "\n",
    "# Convert probabilities to binary labels\n",
    "predicted_labels = (predictions > 0.5).float()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predicted_labels == y_test_tensor).float().mean().item()\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8525\n",
      "Sample 1: Risk Probability = 0.0217, Predicted Risk = Low\n",
      "Sample 2: Risk Probability = 0.0447, Predicted Risk = Low\n",
      "Sample 3: Risk Probability = 0.0137, Predicted Risk = Low\n",
      "Sample 4: Risk Probability = 0.8626, Predicted Risk = High\n",
      "Sample 5: Risk Probability = 0.1896, Predicted Risk = Low\n",
      "Sample 6: Risk Probability = 0.0385, Predicted Risk = Low\n",
      "Sample 7: Risk Probability = 0.9175, Predicted Risk = High\n",
      "Sample 8: Risk Probability = 0.1933, Predicted Risk = Low\n",
      "Sample 9: Risk Probability = 0.9835, Predicted Risk = High\n",
      "Sample 10: Risk Probability = 0.5012, Predicted Risk = High\n",
      "Sample 11: Risk Probability = 0.1369, Predicted Risk = Low\n",
      "Sample 12: Risk Probability = 0.5872, Predicted Risk = High\n",
      "Sample 13: Risk Probability = 0.0494, Predicted Risk = Low\n",
      "Sample 14: Risk Probability = 0.9426, Predicted Risk = High\n",
      "Sample 15: Risk Probability = 0.9632, Predicted Risk = High\n",
      "Sample 16: Risk Probability = 0.7241, Predicted Risk = High\n",
      "Sample 17: Risk Probability = 0.9655, Predicted Risk = High\n",
      "Sample 18: Risk Probability = 0.8077, Predicted Risk = High\n",
      "Sample 19: Risk Probability = 0.8179, Predicted Risk = High\n",
      "Sample 20: Risk Probability = 0.8445, Predicted Risk = High\n",
      "Sample 21: Risk Probability = 0.9297, Predicted Risk = High\n",
      "Sample 22: Risk Probability = 0.4535, Predicted Risk = Low\n",
      "Sample 23: Risk Probability = 0.9557, Predicted Risk = High\n",
      "Sample 24: Risk Probability = 0.7960, Predicted Risk = High\n",
      "Sample 25: Risk Probability = 0.9634, Predicted Risk = High\n",
      "Sample 26: Risk Probability = 0.0201, Predicted Risk = Low\n",
      "Sample 27: Risk Probability = 0.1631, Predicted Risk = Low\n",
      "Sample 28: Risk Probability = 0.5772, Predicted Risk = High\n",
      "Sample 29: Risk Probability = 0.0612, Predicted Risk = Low\n",
      "Sample 30: Risk Probability = 0.8819, Predicted Risk = High\n",
      "Sample 31: Risk Probability = 0.0430, Predicted Risk = Low\n",
      "Sample 32: Risk Probability = 0.8500, Predicted Risk = High\n",
      "Sample 33: Risk Probability = 0.4700, Predicted Risk = Low\n",
      "Sample 34: Risk Probability = 0.0506, Predicted Risk = Low\n",
      "Sample 35: Risk Probability = 0.0426, Predicted Risk = Low\n",
      "Sample 36: Risk Probability = 0.5698, Predicted Risk = High\n",
      "Sample 37: Risk Probability = 0.2702, Predicted Risk = Low\n",
      "Sample 38: Risk Probability = 0.9618, Predicted Risk = High\n",
      "Sample 39: Risk Probability = 0.9653, Predicted Risk = High\n",
      "Sample 40: Risk Probability = 0.4281, Predicted Risk = Low\n",
      "Sample 41: Risk Probability = 0.2196, Predicted Risk = Low\n",
      "Sample 42: Risk Probability = 0.9346, Predicted Risk = High\n",
      "Sample 43: Risk Probability = 0.9811, Predicted Risk = High\n",
      "Sample 44: Risk Probability = 0.7800, Predicted Risk = High\n",
      "Sample 45: Risk Probability = 0.0531, Predicted Risk = Low\n",
      "Sample 46: Risk Probability = 0.4304, Predicted Risk = Low\n",
      "Sample 47: Risk Probability = 0.9671, Predicted Risk = High\n",
      "Sample 48: Risk Probability = 0.5491, Predicted Risk = High\n",
      "Sample 49: Risk Probability = 0.8281, Predicted Risk = High\n",
      "Sample 50: Risk Probability = 0.8447, Predicted Risk = High\n",
      "Sample 51: Risk Probability = 0.4856, Predicted Risk = Low\n",
      "Sample 52: Risk Probability = 0.9391, Predicted Risk = High\n",
      "Sample 53: Risk Probability = 0.6190, Predicted Risk = High\n",
      "Sample 54: Risk Probability = 0.0104, Predicted Risk = Low\n",
      "Sample 55: Risk Probability = 0.9559, Predicted Risk = High\n",
      "Sample 56: Risk Probability = 0.9853, Predicted Risk = High\n",
      "Sample 57: Risk Probability = 0.9696, Predicted Risk = High\n",
      "Sample 58: Risk Probability = 0.8340, Predicted Risk = High\n",
      "Sample 59: Risk Probability = 0.0202, Predicted Risk = Low\n",
      "Sample 60: Risk Probability = 0.0850, Predicted Risk = Low\n",
      "Sample 61: Risk Probability = 0.9418, Predicted Risk = High\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Prepare the categorical and numerical test data\n",
    "X_cat_test_tensor = torch.tensor(X_test[categorical_cols].values, dtype=torch.long).to(device)\n",
    "X_num_test_tensor = torch.tensor(X_test[numerical_cols].values, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_cat_test_tensor, X_num_test_tensor)\n",
    "    risk_probabilities = torch.sigmoid(predictions)  # Convert to probabilities\n",
    "\n",
    "# Convert probabilities to binary labels\n",
    "predicted_labels = (risk_probabilities >= 0.5).float()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predicted_labels == y_test_tensor).float().mean().item()\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print individual risk predictions\n",
    "for i, prob in enumerate(risk_probabilities):\n",
    "    print(f\"Sample {i+1}: Risk Probability = {prob.item():.4f}, Predicted Risk = {'High' if predicted_labels[i] else 'Low'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually Computed Accuracy: 0.8525\n"
     ]
    }
   ],
   "source": [
    "# Convert risk probabilities into binary labels\n",
    "predicted_labels = (risk_probabilities >= 0.5).float()\n",
    "\n",
    "# Compare predictions with actual values\n",
    "correct_predictions = (predicted_labels == y_test_tensor).sum().item()\n",
    "total_samples = y_test_tensor.shape[0]\n",
    "\n",
    "# Compute Accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Manually Computed Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Actual = Low, Predicted = Low, Probability = 0.0231\n",
      "Sample 2: Actual = Low, Predicted = Low, Probability = 0.0856\n",
      "Sample 3: Actual = Low, Predicted = Low, Probability = 0.0139\n",
      "Sample 4: Actual = Low, Predicted = High, Probability = 0.9112\n",
      "Sample 5: Actual = Low, Predicted = Low, Probability = 0.3217\n",
      "Sample 6: Actual = Low, Predicted = Low, Probability = 0.0587\n",
      "Sample 7: Actual = High, Predicted = High, Probability = 0.9346\n",
      "Sample 8: Actual = Low, Predicted = Low, Probability = 0.2496\n",
      "Sample 9: Actual = High, Predicted = High, Probability = 0.9798\n",
      "Sample 10: Actual = Low, Predicted = Low, Probability = 0.4503\n",
      "Sample 11: Actual = High, Predicted = Low, Probability = 0.1489\n",
      "Sample 12: Actual = High, Predicted = High, Probability = 0.6658\n",
      "Sample 13: Actual = Low, Predicted = Low, Probability = 0.0605\n",
      "Sample 14: Actual = High, Predicted = High, Probability = 0.9437\n",
      "Sample 15: Actual = High, Predicted = High, Probability = 0.9607\n",
      "Sample 16: Actual = High, Predicted = High, Probability = 0.6979\n",
      "Sample 17: Actual = High, Predicted = High, Probability = 0.9725\n",
      "Sample 18: Actual = High, Predicted = High, Probability = 0.7940\n",
      "Sample 19: Actual = High, Predicted = High, Probability = 0.8273\n",
      "Sample 20: Actual = High, Predicted = High, Probability = 0.7161\n",
      "Sample 21: Actual = Low, Predicted = High, Probability = 0.9383\n",
      "Sample 22: Actual = Low, Predicted = Low, Probability = 0.3567\n",
      "Sample 23: Actual = High, Predicted = High, Probability = 0.9554\n",
      "Sample 24: Actual = Low, Predicted = High, Probability = 0.7594\n",
      "Sample 25: Actual = High, Predicted = High, Probability = 0.9702\n",
      "Sample 26: Actual = Low, Predicted = Low, Probability = 0.0256\n",
      "Sample 27: Actual = Low, Predicted = Low, Probability = 0.1850\n",
      "Sample 28: Actual = High, Predicted = High, Probability = 0.6796\n",
      "Sample 29: Actual = Low, Predicted = Low, Probability = 0.0845\n",
      "Sample 30: Actual = High, Predicted = High, Probability = 0.8953\n",
      "Sample 31: Actual = Low, Predicted = Low, Probability = 0.0570\n",
      "Sample 32: Actual = High, Predicted = High, Probability = 0.8798\n",
      "Sample 33: Actual = High, Predicted = High, Probability = 0.5784\n",
      "Sample 34: Actual = Low, Predicted = Low, Probability = 0.0462\n",
      "Sample 35: Actual = Low, Predicted = Low, Probability = 0.0279\n",
      "Sample 36: Actual = High, Predicted = High, Probability = 0.6928\n",
      "Sample 37: Actual = Low, Predicted = Low, Probability = 0.2812\n",
      "Sample 38: Actual = High, Predicted = High, Probability = 0.9676\n",
      "Sample 39: Actual = High, Predicted = High, Probability = 0.9663\n",
      "Sample 40: Actual = High, Predicted = Low, Probability = 0.3964\n",
      "Sample 41: Actual = Low, Predicted = Low, Probability = 0.3491\n",
      "Sample 42: Actual = High, Predicted = High, Probability = 0.8672\n",
      "Sample 43: Actual = High, Predicted = High, Probability = 0.9799\n",
      "Sample 44: Actual = High, Predicted = High, Probability = 0.7547\n",
      "Sample 45: Actual = Low, Predicted = Low, Probability = 0.0658\n",
      "Sample 46: Actual = Low, Predicted = High, Probability = 0.5823\n",
      "Sample 47: Actual = High, Predicted = High, Probability = 0.9526\n",
      "Sample 48: Actual = Low, Predicted = High, Probability = 0.6916\n",
      "Sample 49: Actual = High, Predicted = High, Probability = 0.7945\n",
      "Sample 50: Actual = Low, Predicted = High, Probability = 0.8665\n",
      "Sample 51: Actual = Low, Predicted = High, Probability = 0.5434\n",
      "Sample 52: Actual = High, Predicted = High, Probability = 0.9589\n",
      "Sample 53: Actual = High, Predicted = High, Probability = 0.5592\n",
      "Sample 54: Actual = Low, Predicted = Low, Probability = 0.0116\n",
      "Sample 55: Actual = High, Predicted = High, Probability = 0.9504\n",
      "Sample 56: Actual = High, Predicted = High, Probability = 0.9853\n",
      "Sample 57: Actual = High, Predicted = High, Probability = 0.9551\n",
      "Sample 58: Actual = High, Predicted = High, Probability = 0.7605\n",
      "Sample 59: Actual = Low, Predicted = Low, Probability = 0.0206\n",
      "Sample 60: Actual = Low, Predicted = Low, Probability = 0.0859\n",
      "Sample 61: Actual = High, Predicted = High, Probability = 0.9421\n"
     ]
    }
   ],
   "source": [
    "# Compare predicted vs actual risk labels\n",
    "for i in range(len(y_test_tensor)):\n",
    "    actual_label = \"High\" if y_test_tensor[i] >= 0.5 else \"Low\"\n",
    "    predicted_label = \"High\" if predicted_labels[i] >= 0.5 else \"Low\"\n",
    "    print(f\"Sample {i+1}: Actual = {actual_label}, Predicted = {predicted_label}, Probability = {risk_probabilities[i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179    0\n",
      "197    0\n",
      "285    0\n",
      "194    0\n",
      "188    0\n",
      "240    0\n",
      "160    1\n",
      "167    0\n",
      "136    1\n",
      "228    0\n",
      "Name: target, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:10])  # Print the first 10 values to verify\n",
    "print(type(y_test))  # Check the type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('embeddings.0.weight', tensor([[-0.4665, -0.3981,  0.4015, -0.7564,  0.0261, -0.6146,  0.0256,  0.3997,\n",
      "          0.3126, -3.5598],\n",
      "        [ 0.4571, -0.0866,  0.6526, -2.6397, -0.7126,  0.1597, -0.4988,  0.7018,\n",
      "         -0.1055, -1.3123]])), ('embeddings.1.weight', tensor([[ 0.7352,  2.1559, -0.6376,  0.8650, -0.2623,  0.7231,  0.8421,  0.5469,\n",
      "         -1.0590,  1.7783],\n",
      "        [ 0.4798,  1.7228, -0.6604,  1.1502,  1.2079, -0.8260, -1.1355, -0.7072,\n",
      "          0.4641, -0.0668],\n",
      "        [ 1.0848, -2.4661, -0.3843, -1.5720, -1.1986,  0.1252, -0.2154, -0.7849,\n",
      "          0.7627,  0.8918],\n",
      "        [-0.4974,  0.7022,  0.7682,  0.0310,  0.8382, -0.1459, -0.3161,  0.2317,\n",
      "         -0.1350,  0.7918]])), ('embeddings.2.weight', tensor([[-0.2937, -0.4032,  3.0801, -0.4451,  1.2618, -0.1556,  0.1131, -0.5020,\n",
      "          1.1422, -2.3514],\n",
      "        [-0.6750, -1.1379,  1.4064, -1.5448,  0.0377, -0.4571, -0.2649,  0.0089,\n",
      "          1.3361,  1.0939]])), ('embeddings.3.weight', tensor([[ 1.5328,  1.9570,  0.7423, -1.2049,  0.0260, -2.5331, -1.3074, -1.6757,\n",
      "         -0.2772,  0.5404],\n",
      "        [-1.3301, -0.4399, -0.3250, -1.2002, -2.7387, -1.6778,  0.1777, -0.6740,\n",
      "         -0.7617,  0.7459],\n",
      "        [ 0.7271,  0.5336, -1.8035,  1.7443,  0.1222,  0.8503, -0.1240, -0.5888,\n",
      "         -0.7556,  0.2270]])), ('embeddings.4.weight', tensor([[-1.6310,  1.1415,  1.3696, -0.1961, -0.9318,  0.4582,  2.0039,  0.2748,\n",
      "         -1.3970, -2.1284],\n",
      "        [ 1.4514,  0.6172,  1.9458,  0.9755, -1.3871, -1.2885, -0.9763,  1.3071,\n",
      "          0.3839,  2.0124]])), ('embeddings.5.weight', tensor([[-0.2520,  0.7590, -1.1237, -0.6208,  0.7520,  1.4025,  2.2162, -0.6375,\n",
      "          0.4871,  0.4458],\n",
      "        [-0.9160, -0.7887, -0.7673, -0.7611,  0.9661, -1.3762, -1.0124,  0.4265,\n",
      "         -0.5132,  1.5348],\n",
      "        [ 0.3587,  0.1163, -0.5989, -0.3158,  0.6299, -0.4563, -0.0085,  0.6371,\n",
      "          0.9345,  0.4054]])), ('embeddings.6.weight', tensor([[-7.3251e-01, -8.5049e-01,  2.0347e-03,  1.4790e+00, -5.7322e-01,\n",
      "         -6.9824e-01,  1.2910e+00, -4.3633e-01,  3.0163e-01, -9.9546e-01],\n",
      "        [-4.0050e-01,  9.1287e-01,  6.6862e-01, -8.1357e-01,  1.0204e-06,\n",
      "         -1.2152e+00, -3.2879e+00,  1.1732e+00,  2.7243e-01,  5.3151e-01],\n",
      "        [ 8.3807e-01,  9.7150e-01, -2.0138e+00, -7.1552e-01, -3.6565e-01,\n",
      "          3.2795e-01,  4.8303e-01,  7.2403e-01, -4.4854e-01,  8.9790e-01],\n",
      "        [ 3.7046e-01, -1.5364e+00,  1.5890e+00,  1.3946e+00,  2.0468e+00,\n",
      "         -5.8602e-01,  2.5946e-01,  5.0373e-01,  1.5744e+00, -1.5257e+00],\n",
      "        [ 1.1773e+00,  1.2032e+00, -2.9885e-01, -5.4523e-01,  9.0232e-01,\n",
      "         -1.2086e+00,  5.5838e-01, -1.7800e+00, -2.0767e+00,  1.2108e+00]])), ('embeddings.7.weight', tensor([[ 0.4306,  0.3364,  0.0143,  1.4329,  0.2688, -0.7863, -0.0569, -1.4419,\n",
      "         -1.0140,  1.0050],\n",
      "        [-0.7432,  0.1727,  1.1237,  1.0917, -1.7089, -1.6003,  3.1985,  0.2575,\n",
      "         -0.1722,  0.7177],\n",
      "        [-0.1559, -0.5365, -0.5339,  0.3212, -1.2083,  0.9923, -0.5997, -0.4298,\n",
      "         -0.5760, -2.2805],\n",
      "        [-0.3276, -0.1781, -0.1142,  2.1725, -0.1641,  2.8451, -0.1151, -0.1465,\n",
      "          1.9950, -0.5999]])), ('gru.weight_ih_l0', tensor([[-0.1672, -0.1624, -0.0387,  ..., -0.2039, -0.0691, -0.1120],\n",
      "        [ 0.0182,  0.1566, -0.0658,  ...,  0.0455, -0.0544,  0.1553],\n",
      "        [-0.1231, -0.0824, -0.1059,  ..., -0.0222,  0.0014, -0.1870],\n",
      "        ...,\n",
      "        [-0.0718,  0.0189, -0.0192,  ...,  0.0666,  0.0735,  0.2210],\n",
      "        [ 0.1004, -0.0298,  0.1300,  ...,  0.0314, -0.0759, -0.1182],\n",
      "        [-0.0042,  0.0054, -0.1351,  ..., -0.0376,  0.0399, -0.0824]])), ('gru.weight_hh_l0', tensor([[ 0.0089,  0.1139, -0.0041,  ..., -0.0567, -0.0628, -0.0603],\n",
      "        [ 0.0055, -0.0424,  0.1077,  ...,  0.0170, -0.0379,  0.0167],\n",
      "        [ 0.0098, -0.0973,  0.1173,  ..., -0.0196, -0.0893,  0.0858],\n",
      "        ...,\n",
      "        [ 0.0783, -0.0556,  0.1079,  ..., -0.1109,  0.0669, -0.0674],\n",
      "        [-0.1131,  0.0545,  0.0927,  ...,  0.0062, -0.0146, -0.1206],\n",
      "        [-0.0968,  0.0092,  0.0163,  ..., -0.1148,  0.0646,  0.0609]])), ('gru.bias_ih_l0', tensor([-0.0146,  0.0065,  0.0734,  0.1138,  0.0166, -0.0429, -0.0305,  0.0802,\n",
      "        -0.0668, -0.0535,  0.0922,  0.0967,  0.0335, -0.0551, -0.0722,  0.0090,\n",
      "        -0.0198, -0.0505, -0.0490, -0.0841,  0.0684, -0.1002, -0.0419, -0.1337,\n",
      "         0.0200,  0.0185, -0.0174, -0.0501,  0.0697,  0.0586,  0.0472, -0.0408,\n",
      "         0.0661,  0.0189, -0.0248, -0.0681,  0.0663, -0.0727,  0.0552, -0.0324,\n",
      "        -0.0340, -0.1420, -0.0005, -0.0131, -0.0163,  0.0941, -0.0701, -0.0648,\n",
      "        -0.1216,  0.0923, -0.0268,  0.0932, -0.0784, -0.1180,  0.0982,  0.1107,\n",
      "         0.0586, -0.1139,  0.1052,  0.0474, -0.0217, -0.1256,  0.0266, -0.0598,\n",
      "        -0.0163, -0.1452, -0.1659,  0.0524,  0.0530, -0.0021,  0.0680, -0.0332,\n",
      "        -0.0314,  0.0389,  0.0124,  0.0539, -0.0835, -0.1602, -0.0755, -0.0543,\n",
      "        -0.0511,  0.0882, -0.0424, -0.0744,  0.0907, -0.0709, -0.0971, -0.0432,\n",
      "         0.0115,  0.0221,  0.0594,  0.0713, -0.0236, -0.0359,  0.0190, -0.1307,\n",
      "         0.0207,  0.0843,  0.0579, -0.0928, -0.0509, -0.0969,  0.0457,  0.0107,\n",
      "        -0.1205, -0.1106,  0.0585, -0.0794, -0.0949, -0.1141, -0.0255,  0.0495,\n",
      "         0.0082, -0.0133,  0.0293,  0.0386, -0.1497, -0.0034, -0.1171, -0.0107,\n",
      "         0.0210, -0.1167, -0.1337,  0.0477, -0.0554, -0.0852, -0.0943, -0.0825,\n",
      "        -0.0681, -0.1281, -0.0498, -0.1142,  0.0565,  0.0546, -0.0486, -0.0124,\n",
      "        -0.0952, -0.1420, -0.0063, -0.0787,  0.0691, -0.0660, -0.1047,  0.1194,\n",
      "         0.0609, -0.0650,  0.0105,  0.0775, -0.1055, -0.0030,  0.0841,  0.0506,\n",
      "         0.0600, -0.0295,  0.0750, -0.1081, -0.1033,  0.0462,  0.0737, -0.1019,\n",
      "        -0.1230,  0.0804, -0.0965,  0.1112,  0.0337,  0.0962,  0.1224, -0.0420,\n",
      "        -0.1073,  0.0816,  0.0507,  0.0589, -0.0334,  0.0487,  0.1064, -0.0442,\n",
      "         0.0289,  0.0089,  0.0871,  0.0716,  0.0511, -0.1038,  0.0627, -0.0449,\n",
      "         0.1000,  0.0285, -0.0819,  0.0486, -0.1013, -0.0347, -0.0390,  0.0486])), ('gru.bias_hh_l0', tensor([-6.5309e-03, -5.3042e-02,  2.1695e-02,  7.3096e-02, -5.3071e-02,\n",
      "        -5.0381e-02,  8.4323e-02,  2.7712e-02, -2.5901e-02, -1.0258e-01,\n",
      "         2.7862e-04, -9.5170e-02, -1.6267e-03,  2.0400e-02,  3.0312e-02,\n",
      "        -6.9680e-02,  6.9532e-02, -6.2636e-02,  2.7460e-02, -9.9005e-02,\n",
      "        -2.6597e-02,  2.2861e-02,  1.2932e-01, -9.4966e-03, -4.1038e-02,\n",
      "        -3.6639e-02,  7.3802e-02,  6.2241e-02,  5.0964e-03, -6.3704e-02,\n",
      "         9.2626e-02, -7.8796e-02,  2.4111e-03, -4.9188e-02, -5.1462e-03,\n",
      "         5.0409e-02, -9.6173e-02, -2.9787e-02, -4.6689e-02, -1.0532e-01,\n",
      "         1.3637e-02,  3.9566e-02,  1.0092e-01,  1.0705e-01, -3.5515e-02,\n",
      "         9.3326e-02,  1.8018e-02, -1.9746e-02, -1.4271e-01, -7.1905e-02,\n",
      "        -8.0205e-02, -1.4851e-04, -2.1725e-02, -3.8667e-05, -6.5835e-02,\n",
      "        -3.6363e-02,  7.8280e-03, -2.3752e-02,  5.9561e-02, -8.3429e-02,\n",
      "        -7.5004e-02, -9.6390e-02,  4.3678e-02,  5.3873e-02,  7.0400e-02,\n",
      "        -1.4465e-01, -1.2497e-01,  4.8882e-02, -1.0484e-01, -1.3625e-01,\n",
      "        -1.0920e-01,  4.3574e-02,  6.8254e-02, -1.1837e-02,  1.2098e-01,\n",
      "        -7.0830e-02,  6.5013e-02, -1.1457e-02, -1.4267e-01, -5.5515e-02,\n",
      "        -9.5103e-03,  3.9575e-02, -1.5135e-01,  4.6468e-02, -2.6347e-02,\n",
      "        -1.2531e-01, -2.7220e-02, -2.8201e-02,  2.6609e-02, -1.2736e-01,\n",
      "        -5.5148e-02,  1.7180e-02, -8.9226e-02,  1.1519e-01, -6.8439e-02,\n",
      "         7.9501e-02,  4.8062e-02,  6.5908e-02, -2.7818e-03, -4.5863e-02,\n",
      "        -1.9149e-02, -2.5390e-02, -1.0068e-01,  3.1451e-02, -6.9647e-02,\n",
      "         3.3504e-02, -1.3766e-01, -5.7655e-02, -2.3785e-02, -3.9241e-02,\n",
      "        -1.3592e-01, -4.8316e-02,  4.0363e-02,  7.5718e-02, -3.8374e-02,\n",
      "        -1.2360e-01,  5.1981e-02,  7.8340e-02, -4.6028e-02,  4.7349e-02,\n",
      "        -1.7250e-04, -1.1024e-01, -4.8885e-03, -1.1970e-01, -4.8128e-02,\n",
      "         2.7654e-03, -5.3100e-02,  7.7518e-02, -4.2210e-02,  1.1690e-01,\n",
      "         1.3330e-01, -1.0084e-01,  7.3088e-02,  6.5024e-02,  1.2335e-01,\n",
      "        -5.0480e-02, -5.6919e-02, -1.3879e-01,  1.0531e-01,  1.1272e-01,\n",
      "        -1.0258e-01, -1.1831e-01, -5.9850e-02, -2.9537e-02,  9.5002e-02,\n",
      "         1.2171e-01,  9.6049e-02, -1.4187e-01, -1.2805e-01, -1.4842e-01,\n",
      "         9.1845e-02,  1.2004e-01, -1.1461e-01, -7.5469e-02,  3.7056e-02,\n",
      "        -7.7058e-02,  1.0098e-01,  9.2205e-02,  1.2406e-01,  3.0728e-02,\n",
      "         8.1235e-02,  9.2094e-02, -1.2341e-01,  3.4115e-02, -1.2491e-02,\n",
      "        -9.3428e-02, -9.4353e-02,  3.8542e-02,  1.0491e-01, -9.6334e-02,\n",
      "        -1.2638e-01, -1.2804e-01,  2.6606e-02, -1.1410e-01,  1.0950e-02,\n",
      "        -1.3918e-01,  2.1939e-02,  1.7274e-02,  1.1224e-01,  1.6191e-02,\n",
      "        -7.8440e-02, -7.4035e-02,  2.5828e-02,  4.9841e-02, -1.0549e-01,\n",
      "         9.6445e-02,  9.4347e-02, -7.0456e-02,  1.0620e-01,  5.1112e-02,\n",
      "        -9.0383e-02,  1.4739e-01])), ('fc.weight', tensor([[-0.1777, -0.0714,  0.1215, -0.1670, -0.0565, -0.0428,  0.0601, -0.1315,\n",
      "          0.0986, -0.1233,  0.0741, -0.1113,  0.1759, -0.1742, -0.1653,  0.1612,\n",
      "          0.1638, -0.1194, -0.0926,  0.1698, -0.1447,  0.1661,  0.0894,  0.1080,\n",
      "         -0.0473,  0.0662, -0.0774,  0.0022,  0.0312, -0.0960,  0.1107,  0.0821,\n",
      "         -0.0903, -0.1050, -0.0808, -0.1152,  0.1885, -0.1499,  0.1869, -0.0760,\n",
      "          0.0900,  0.0722, -0.0719, -0.1069,  0.1055,  0.1014,  0.0849, -0.1451,\n",
      "         -0.1337, -0.1072, -0.1436, -0.1197, -0.0850,  0.1325, -0.1524,  0.0545,\n",
      "         -0.1459, -0.1176,  0.0949, -0.0959, -0.0874, -0.0650,  0.1131,  0.0533]])), ('fc.bias', tensor([-0.0229]))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "fullpath = \"mymodel.pkl\"\n",
    "\n",
    "# Try loading the model with weights_only=False\n",
    "model = torch.load(fullpath, weights_only=False)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'ca': [0 2 1 3 4]\n",
      "Unique values in 'thal': [1 2 3 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Add this to your training code\n",
    "print(\"Unique values in 'ca':\", df['ca'].unique())\n",
    "print(\"Unique values in 'thal':\", df['thal'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
